{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif __name__ == \"__main__\":\\n    pdf_filename = \"example.pdf\"  # Replace with the actual PDF filename\\n    txt_filename = \"output.txt\"    # Replace with the desired output text file name\\n    extract_text_from_pdf(pdf_filename, txt_filename)\\n    '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "def extract_text_from_pdf(pdf_filename, txt_filename):\n",
    "    try:\n",
    "        text = extract_text(pdf_filename)\n",
    "        with open(txt_filename, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "            txt_file.write(text)\n",
    "        print(f\"Text extracted from '{pdf_filename}' and saved as '{txt_filename}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "'''\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_filename = \"example.pdf\"  # Replace with the actual PDF filename\n",
    "    txt_filename = \"output.txt\"    # Replace with the desired output text file name\n",
    "    extract_text_from_pdf(pdf_filename, txt_filename)\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Example usage:\\nif __name__ == \"__main__\":\\n    pdf_filename = \"example.pdf\"  # Replace with the actual PDF filename\\n    txt_filename = \"output.txt\"    # Replace with the desired output text file name\\n    extract_and_clean_text_from_pdf(pdf_filename, txt_filename)\\n    '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "import codecs\n",
    "\n",
    "def extract_text_from_pdf(pdf_filename, txt_filename):\n",
    "    try:\n",
    "        # Extract raw text from the PDF\n",
    "        raw_text = extract_text(pdf_filename)\n",
    "\n",
    "        # Use UTF-8 encoding to decode the text (assuming UTF-8 is common)\n",
    "        decoded_text = raw_text.encode('utf-8', 'ignore').decode('utf-8')\n",
    "\n",
    "        # Clean the text by removing extra spaces and non-breaking spaces\n",
    "        cleaned_text = ' '.join(decoded_text.split())\n",
    "\n",
    "        # Save the cleaned text to the specified text file\n",
    "        with codecs.open(txt_filename, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "            txt_file.write(cleaned_text)\n",
    "\n",
    "        print(f\"Text extracted, cleaned, and saved as '{txt_filename}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "'''\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_filename = \"example.pdf\"  # Replace with the actual PDF filename\n",
    "    txt_filename = \"output.txt\"    # Replace with the desired output text file name\n",
    "    extract_and_clean_text_from_pdf(pdf_filename, txt_filename)\n",
    "    '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slice Methods Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/paria/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Example usage:\\nif __name__ == \"__main__\":\\n    input_filename = \"output.txt\"  # Replace with the actual input text file\\n    output_filename = \"methods.txt\"  # Name of the output file\\n    extract_methods_to_results(input_filename, output_filename)\\n    '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def extract_methods_to_results(input_filename, output_filename):\n",
    "    try:\n",
    "        with open(input_filename, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Tokenize the text into sentences\n",
    "        sentences = sent_tokenize(text)\n",
    "\n",
    "        # Initialize variables to track the start and end indices\n",
    "        start_index = None\n",
    "        end_index = None\n",
    "\n",
    "        # Define a regex pattern to match any of the specified keywords (case-insensitive)\n",
    "        keywords_pattern = re.compile(r\"\\b(?:Methods|Methodology|Method|Methodologies)\\b\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "        # Iterate through the sentences to find the relevant section\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if re.search(keywords_pattern, sentence):\n",
    "                start_index = i\n",
    "                break  # Stop when a keyword is found\n",
    "\n",
    "        # If the start index was found, search for the end index\n",
    "        if start_index is not None:\n",
    "            for i in range(start_index + 1, len(sentences)):\n",
    "                if \"Result\" in sentences[i]:\n",
    "                    end_index = i\n",
    "                    break  # Stop when \"Result\" is found\n",
    "\n",
    "        # If both start and end indices are found, extract the text between them\n",
    "        if start_index is not None and end_index is not None:\n",
    "            extracted_text = ' '.join(sentences[start_index+1:end_index])\n",
    "            with open(output_filename, \"w\", encoding=\"utf-8\") as output_file:\n",
    "                output_file.write(extracted_text)\n",
    "            print(f\"Extracted text saved as '{output_filename}'.\")\n",
    "        else:\n",
    "            print(\"Section not found\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found\")\n",
    "'''\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    input_filename = \"output.txt\"  # Replace with the actual input text file\n",
    "    output_filename = \"methods.txt\"  # Name of the output file\n",
    "    extract_methods_to_results(input_filename, output_filename)\n",
    "    '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreaction of Methods Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/paria/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def extract_and_generate_sentences(input_filename, output_filename):\n",
    "    try:\n",
    "        with open(input_filename, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Tokenize the text into sentences\n",
    "        sentences = sent_tokenize(text)\n",
    "\n",
    "        # Initialize a list to store cleaned and relevant sentences\n",
    "        relevant_sentences = []\n",
    "\n",
    "        # Define keywords to search for\n",
    "        keywords = [\"experiment\", \"survey\", \"interview\"]\n",
    "\n",
    "        # Define a function to clean up a sentence\n",
    "        def clean_sentence(sentence):\n",
    "            # Remove leading/trailing whitespace and extra spaces\n",
    "            cleaned_sentence = ' '.join(sentence.split())\n",
    "            return cleaned_sentence\n",
    "\n",
    "        # Iterate through the sentences to find relevant sentences\n",
    "        for sentence in sentences:\n",
    "            for keyword in keywords:\n",
    "                if keyword in sentence.lower():\n",
    "                    cleaned_sentence = clean_sentence(re.sub(r'\\b' + re.escape(keyword) + r'\\b', '', sentence, flags=re.I))\n",
    "                    if cleaned_sentence:  # Check if the cleaned sentence is not empty\n",
    "                        relevant_sentences.append(f\"{keyword.capitalize()}: {cleaned_sentence}\")\n",
    "\n",
    "        # Generate the output text\n",
    "        output_text = \"\\n\".join(relevant_sentences)\n",
    "\n",
    "        # Save the output text to the specified file\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as output_file:\n",
    "            output_file.write(output_text)\n",
    "\n",
    "        print(f\"Relevant sentences saved in '{output_filename}'.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found\")\n",
    "        '''\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    input_filename = \"methods.txt\"  # Replace with the actual input text file\n",
    "    output_filename = \"relevant_sentences.txt\"  # Name of the output file\n",
    "    extract_and_generate_sentences(input_filename, output_filename)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extracted, cleaned, and saved as 'output_results/The Use of Artificial Intelligence with Students with Identified Disabilities  A Systematic Review with Critique/output.txt'.\n",
      "Section not found\n",
      "File not found\n",
      "Text extracted, cleaned, and saved as 'output_results/WilsonFutureEng/output.txt'.\n",
      "Extracted text saved as 'output_results/WilsonFutureEng/methods.txt'.\n",
      "Relevant sentences saved in 'output_results/WilsonFutureEng/relevant_sentences.txt'.\n",
      "Text extracted, cleaned, and saved as 'output_results/literature-review-on-disability-participation-in-the-engineering-field/output.txt'.\n",
      "Extracted text saved as 'output_results/literature-review-on-disability-participation-in-the-engineering-field/methods.txt'.\n",
      "Relevant sentences saved in 'output_results/literature-review-on-disability-participation-in-the-engineering-field/relevant_sentences.txt'.\n",
      "Text extracted, cleaned, and saved as 'output_results/missing-from-the-classroom-current-representations-of-disability-in-engineering-education/output.txt'.\n",
      "Section not found\n",
      "File not found\n",
      "Text extracted, cleaned, and saved as 'output_results/Intelligent Tutoring System in Education for Disabled Learners Using Human Computer Interaction and Augmented Reality /output.txt'.\n",
      "Extracted text saved as 'output_results/Intelligent Tutoring System in Education for Disabled Learners Using Human Computer Interaction and Augmented Reality /methods.txt'.\n",
      "Relevant sentences saved in 'output_results/Intelligent Tutoring System in Education for Disabled Learners Using Human Computer Interaction and Augmented Reality /relevant_sentences.txt'.\n",
      "Text extracted, cleaned, and saved as 'output_results/ChatBot4AgingCogDis/output.txt'.\n",
      "Extracted text saved as 'output_results/ChatBot4AgingCogDis/methods.txt'.\n",
      "Relevant sentences saved in 'output_results/ChatBot4AgingCogDis/relevant_sentences.txt'.\n",
      "Text extracted, cleaned, and saved as 'output_results/004_Alexa/output.txt'.\n",
      "Extracted text saved as 'output_results/004_Alexa/methods.txt'.\n",
      "Relevant sentences saved in 'output_results/004_Alexa/relevant_sentences.txt'.\n",
      "Text extracted, cleaned, and saved as 'output_results/Exploring student disability and professional identity  navigating sociocultural expectations in U.S. undergraduate civil engineering programs/output.txt'.\n",
      "Section not found\n",
      "File not found\n",
      "Text extracted, cleaned, and saved as 'output_results/frobt-09-1006921/output.txt'.\n",
      "Extracted text saved as 'output_results/frobt-09-1006921/methods.txt'.\n",
      "Relevant sentences saved in 'output_results/frobt-09-1006921/relevant_sentences.txt'.\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Specify the folder containing your PDF files\n",
    "pdf_folder = \"/Users/paria/Desktop/ai/code\"  # Replace with your folder name\n",
    "\n",
    "# Ensure the output folder exists\n",
    "output_folder = \"output_results\"  # Replace with your desired output folder name\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate through PDF files in the folder\n",
    "for root, dirs, files in os.walk(pdf_folder):\n",
    "    for pdf_filename in files:\n",
    "        if pdf_filename.endswith(\".pdf\"):\n",
    "            pdf_filepath = os.path.join(root, pdf_filename)\n",
    "\n",
    "            # Create a folder for the PDF file (without the \".pdf\" extension)\n",
    "            folder_name = os.path.splitext(pdf_filename)[0]\n",
    "            folder_path = os.path.join(output_folder, folder_name)\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "            # Call the existing functions for each PDF file\n",
    "            extract_text_from_pdf(pdf_filepath, os.path.join(folder_path, \"output.txt\"))\n",
    "            extract_methods_to_results(os.path.join(folder_path, \"output.txt\"), os.path.join(folder_path, \"methods.txt\"))\n",
    "            extract_and_generate_sentences(os.path.join(folder_path, \"methods.txt\"), os.path.join(folder_path, \"relevant_sentences.txt\"))\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
